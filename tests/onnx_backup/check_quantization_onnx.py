# inspect_onnx_model.py

import onnx
import numpy as np
import argparse
import os

def find_initializer(model, name):
    """Helper function to find an initializer by name."""
    for init in model.graph.initializer:
        if init.name == name:
            return onnx.numpy_helper.to_array(init)
    return None

def find_node_by_input(model, input_name):
    """Helper function to find the node that consumes a specific tensor."""
    for node in model.graph.node:
        if input_name in node.input:
            return node
    return None

def inspect_onnx_model(model_path):
    """
    Loads a quantized ONNX model and inspects its weights, biases,
    and quantization parameters to verify the format.
    """
    if not os.path.exists(model_path):
        print(f"Error: Model file not found at {model_path}")
        return

    print(f"--- üïµÔ∏è Inspecting ONNX Model: {os.path.basename(model_path)} ---")

    # 1. Load the ONNX model
    model = onnx.load(model_path)
    print("‚úÖ Model loaded successfully.")

    # --- We will inspect the Q-Projection layer from the first attention block ---
    # NOTE: These names are generated by the PyTorch -> ONNX export process.
    # You can find the exact names by visualizing the ONNX graph in a tool like Netron.
    WEIGHT_NAME = "attention_blocks.0.q_proj.weight"
    BIAS_NAME = "attention_blocks.0.q_proj.bias"
    # This is the name of the quantized input tensor to the Q-projection layer
    ACTIVATION_NAME = "attention_blocks.0.q_proj.input_quantized"

    # ==============================================================================
    # ## 1. Inspecting a Weight Tensor
    # ==============================================================================
    print("\n## 1. Inspecting a Weight Tensor")
    print(f"   Target: '{WEIGHT_NAME}'")

    weight_tensor = find_initializer(model, WEIGHT_NAME)

    if weight_tensor is not None:
        print(f"   - Data Type: {weight_tensor.dtype}")
        print(f"   - Shape: {weight_tensor.shape}")
        print(f"   - Value Range: Min={weight_tensor.min()}, Max={weight_tensor.max()}")
        if weight_tensor.dtype == np.int8:
            print("   - ‚úÖ STATUS: Correctly quantized to 8-bit integers.")
        else:
            print("   - ‚ö†Ô∏è STATUS: Weight is NOT in 8-bit format.")
    else:
        print(f"   - ‚ùå ERROR: Weight tensor '{WEIGHT_NAME}' not found.")

    # ==============================================================================
    # ## 2. Inspecting a Bias Tensor
    # ==============================================================================
    print("\n## 2. Inspecting a Bias Tensor")
    print(f"   Target: '{BIAS_NAME}'")
    
    bias_tensor = find_initializer(model, BIAS_NAME)
    
    if bias_tensor is not None:
        print(f"   - Data Type: {bias_tensor.dtype}")
        print(f"   - Shape: {bias_tensor.shape}")
        if bias_tensor.dtype == np.float32:
             print("   - ‚úÖ STATUS: Correctly stored as 32-bit float (as expected).")
        else:
            print(f"   - ‚ö†Ô∏è STATUS: Bias is not in float32 format (is {bias_tensor.dtype}).")
    else:
        print(f"   - ‚ùå ERROR: Bias tensor '{BIAS_NAME}' not found.")

    # ==============================================================================
    # ## 3. Inspecting Quantization Parameters (Scale & Zero-Point)
    # ==============================================================================
    print("\n## 3. Inspecting Quantization Parameters")
    print("   In a QDQ model, the scale and zero-point are inputs to DequantizeLinear nodes.")
    
    # --- Find parameters for the WEIGHT tensor ---
    print(f"\n   --- For Weight '{WEIGHT_NAME}' ---")
    # A DequantizeLinear node consumes the weight tensor. Its other inputs are the scale and zero-point.
    weight_dq_node = find_node_by_input(model, WEIGHT_NAME)
    if weight_dq_node and weight_dq_node.op_type == 'DequantizeLinear':
        scale_name = weight_dq_node.input[1]
        zp_name = weight_dq_node.input[2]
        
        scale_val = find_initializer(model, scale_name)
        zp_val = find_initializer(model, zp_name)
        
        print(f"   - Scale Name: '{scale_name}'")
        print(f"   - Scale Value: {scale_val.item()}")
        print(f"   - Zero-Point Name: '{zp_name}'")
        print(f"   - Zero-Point Value: {zp_val.item()}")
        print("   - ‚úÖ STATUS: Found quantization recipe for the weight.")
    else:
        print("   - ‚ùå ERROR: Could not find the DequantizeLinear node for the weight.")
        
    # --- Find parameters for the ACTIVATION tensor ---
    print(f"\n   --- For Activation '{ACTIVATION_NAME}' ---")
    activation_dq_node = find_node_by_input(model, ACTIVATION_NAME)
    if activation_dq_node and activation_dq_node.op_type == 'DequantizeLinear':
        scale_name = activation_dq_node.input[1]
        zp_name = activation_dq_node.input[2]
        
        scale_val = find_initializer(model, scale_name)
        zp_val = find_initializer(model, zp_name)

        print(f"   - Scale Name: '{scale_name}'")
        print(f"   - Scale Value: {scale_val.item()}")
        print(f"   - Zero-Point Name: '{zp_name}'")
        print(f"   - Zero-Point Value: {zp_val.item()}")
        print("   - ‚úÖ STATUS: Found quantization recipe for the activation.")
    else:
        print("   - ‚ùå ERROR: Could not find the DequantizeLinear node for the activation.")
        
    # ==============================================================================
    # ## 4. Inspecting Model Inputs
    # ==============================================================================
    print("\n## 4. Inspecting Model Inputs")
    for i, model_input in enumerate(model.graph.input):
        # The model's inputs also include the constant initializers (weights, etc.),
        # so we filter to show only the ones that are not initializers.
        if find_initializer(model, model_input.name) is None:
            input_shape = [dim.dim_value or f"dynamic({dim.dim_param})" for dim in model_input.type.tensor_type.shape.dim]
            print(f"   - Input [{i}]: '{model_input.name}'")
            print(f"     - Shape: {input_shape}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Inspect a quantized ONNX model to verify its format.")
    parser.add_argument('--model_path', type=str, required=True, help='Path to the quantized_model.onnx file')
    args = parser.parse_args()
    inspect_onnx_model(args.model_path)