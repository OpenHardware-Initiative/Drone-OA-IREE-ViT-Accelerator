# QAT configuration for fine-tuning

# Device and paths
device = cuda # QAT fine-tuning should be done on a GPU for speed
basedir = /scratch2/agustin/Drone-OA-IREE-ViT-Accelerator
logdir = training/logs
datadir = training # Corrected path to parent folder

# Dataset settings
dataset = data # Assuming you want to fine-tune on the full dataset
val_split = 0.2
short = 0 # Use 0 to train on all data

# --- CORRECTED: Load the pre-trained float model ---
load_checkpoint_qat = True
checkpoint_path = /scratch2/agustin/Drone-OA-IREE-ViT-Accelerator/training/logs/d08_31_t03_44/model_000200.pth

# Training hyperparameters for fine-tuning
lr = 1e-5 # BEST PRACTICE: Use a smaller learning rate (e.g., 1/10th) for fine-tuning
N_eps = 20 # Fine-tuning usually requires fewer epochs, 10-20 is a good start
lr_warmup_epochs = 2
lr_decay = True
save_model_freq = 5
val_freq = 1

# Workspace suffix for QAT runs
ws_suffix = _qat_run_1