{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7760dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/denizonat/REPOS/neuroTUM/Drone-ViT-HW-Accelerator\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "project_root = os.path.abspath(\"..\")  # adjust if needed\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(-1, project_root)\n",
    "print(f\"Project root added to sys.path: {project_root}\")\n",
    "import torch\n",
    "#from .ViTSubmodules import *\n",
    "\n",
    "from models.quantized.quant_ready_LSTMNetVIT import QuantReadyLSTMNetViT\n",
    "from models.quantized.quant_ready_ITAConformerLSTM import QuantReadyITALSTM\n",
    "from models.ITAConformerLSTM import ITALSTM\n",
    "from third_party.vitfly.models.model import LSTMNetVIT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfc1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561acba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_input(traj_len=10):\n",
    "    # Simulate a sequence of depth images (T, 1, 60, 90)\n",
    "    depth_images = torch.randn(traj_len, 1, 60, 90)\n",
    "\n",
    "    # Simulate desired velocities (T, 1)\n",
    "    control_input = torch.rand(traj_len, 1)\n",
    "\n",
    "    # Simulate quaternion orientations (T, 4)\n",
    "    orientation = torch.tensor([[1.0, 0.0, 0.0, 0.0]] * traj_len).float()\n",
    "\n",
    "    return [depth_images, control_input, orientation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa118ec",
   "metadata": {},
   "source": [
    "# 1 Prepering the pretrained weights\n",
    "We need to remove the spectral_norm wrappers from the models since they arent supported by the torch QAT API\n",
    "\n",
    "We start by loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ITALSTM()\n",
    "model.load_state_dict(torch.load(\"../models/pretrained_models/ITALSTM.pth\", map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00e9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_lstm = ITALSTM()\n",
    "quant_ita_lstm = QuantReadyITALSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449ae3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [torch.Size([10, 1, 60, 90]), torch.Size([10, 1]), torch.Size([10, 4])]\n"
     ]
    }
   ],
   "source": [
    "X = generate_dummy_input()\n",
    "print(f\"Input shape: {[x.shape for x in X]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d62dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_out = ita_lstm(X)\n",
    "quant_ita_out = quant_ita_lstm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96f629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ita_out shape: torch.Size([10, 3]), quant_ita_out shape: torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"ita_out shape: {ita_out[0].shape}, quant_ita_out shape: {quant_ita_out[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d81c1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/pubflight/lib/python3.8/site-packages/torch/ao/nn/quantizable/modules/rnn.py:316: UserWarning: dropout option for quantizable LSTM is ignored. If you are training, please, use nn.LSTM version followed by `prepare` step.\n",
      "  warnings.warn(\"dropout option for quantizable LSTM is ignored. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantReadyITALSTM(\n",
       "  (encoder_blocks): ModuleList(\n",
       "    (0): QuantReadyITAEncoderLayer(\n",
       "      (patchMerge): OverlapPatchMerging(\n",
       "        (cn1): Conv2d(\n",
       "          1, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3)\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (layerNorm): LayerNorm(\n",
       "          (32,), eps=1e-05, elementwise_affine=True\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_attn): ModuleList(\n",
       "        (0-1): 2 x ITASelfAttentionWrapper(\n",
       "          (cn1): Conv2d(\n",
       "            32, 32, kernel_size=(8, 8), stride=(8, 8)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (ln1): LayerNorm(\n",
       "            (32,), eps=1e-05, elementwise_affine=True\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (self_attn): MultiheadITAWithRequant(\n",
       "            (q_proj): Linear(\n",
       "              in_features=32, out_features=32, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (k_proj): Linear(\n",
       "              in_features=32, out_features=32, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=32, out_features=32, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=32, out_features=32, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_ffn): ModuleList(\n",
       "        (0-1): 2 x QuantReadyMixFFN(\n",
       "          (mlp1): Linear(\n",
       "            in_features=32, out_features=256, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (depthwise): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=32\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (mlp2): Linear(\n",
       "            in_features=256, out_features=32, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_lNorms): ModuleList(\n",
       "        (0-1): 2 x LayerNorm(\n",
       "          (32,), eps=1e-05, elementwise_affine=True\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (adder): FloatFunctional(\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): QuantReadyITAEncoderLayer(\n",
       "      (patchMerge): OverlapPatchMerging(\n",
       "        (cn1): Conv2d(\n",
       "          32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (layerNorm): LayerNorm(\n",
       "          (64,), eps=1e-05, elementwise_affine=True\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_attn): ModuleList(\n",
       "        (0-1): 2 x ITASelfAttentionWrapper(\n",
       "          (cn1): Conv2d(\n",
       "            64, 64, kernel_size=(4, 4), stride=(4, 4)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (ln1): LayerNorm(\n",
       "            (64,), eps=1e-05, elementwise_affine=True\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (self_attn): MultiheadITAWithRequant(\n",
       "            (q_proj): Linear(\n",
       "              in_features=64, out_features=64, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (k_proj): Linear(\n",
       "              in_features=64, out_features=64, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=64, out_features=64, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=64, out_features=64, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_ffn): ModuleList(\n",
       "        (0-1): 2 x QuantReadyMixFFN(\n",
       "          (mlp1): Linear(\n",
       "            in_features=64, out_features=512, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (depthwise): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (mlp2): Linear(\n",
       "            in_features=512, out_features=64, bias=True\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_lNorms): ModuleList(\n",
       "        (0-1): 2 x LayerNorm(\n",
       "          (64,), eps=1e-05, elementwise_affine=True\n",
       "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (adder): FloatFunctional(\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(\n",
       "    in_features=4608, out_features=512, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (lstm): QuantizableLSTM(\n",
       "    (layers): ModuleList(\n",
       "      (0): _LSTMLayer(\n",
       "        (layer_fw): _LSTMSingleLayer(\n",
       "          (cell): QuantizableLSTMCell(\n",
       "            (igates): Linear(\n",
       "              in_features=517, out_features=512, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (hgates): Linear(\n",
       "              in_features=128, out_features=512, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (gates): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (input_gate): Sigmoid(\n",
       "              (activation_post_process): FixedQParamsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "                (activation_post_process): FixedQParamsObserver()\n",
       "              )\n",
       "            )\n",
       "            (forget_gate): Sigmoid(\n",
       "              (activation_post_process): FixedQParamsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "                (activation_post_process): FixedQParamsObserver()\n",
       "              )\n",
       "            )\n",
       "            (cell_gate): Tanh(\n",
       "              (activation_post_process): FixedQParamsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0078]), zero_point=tensor([128], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "                (activation_post_process): FixedQParamsObserver()\n",
       "              )\n",
       "            )\n",
       "            (output_gate): Sigmoid(\n",
       "              (activation_post_process): FixedQParamsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "                (activation_post_process): FixedQParamsObserver()\n",
       "              )\n",
       "            )\n",
       "            (fgate_cx): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (igate_cgate): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (fgate_cx_igate_cgate): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (ogate_cy): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x _LSTMLayer(\n",
       "        (layer_fw): _LSTMSingleLayer(\n",
       "          (cell): QuantizableLSTMCell(\n",
       "            (igates): Linear(\n",
       "              in_features=128, out_features=512, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (hgates): Linear(\n",
       "              in_features=128, out_features=512, bias=True\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (gates): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (input_gate): Sigmoid(\n",
       "              (activation_post_process): FixedQParamsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "                (activation_post_process): FixedQParamsObserver()\n",
       "              )\n",
       "            )\n",
       "            (forget_gate): Sigmoid(\n",
       "              (activation_post_process): FixedQParamsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "                (activation_post_process): FixedQParamsObserver()\n",
       "              )\n",
       "            )\n",
       "            (cell_gate): Tanh(\n",
       "              (activation_post_process): FixedQParamsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0078]), zero_point=tensor([128], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "                (activation_post_process): FixedQParamsObserver()\n",
       "              )\n",
       "            )\n",
       "            (output_gate): Sigmoid(\n",
       "              (activation_post_process): FixedQParamsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "                (activation_post_process): FixedQParamsObserver()\n",
       "              )\n",
       "            )\n",
       "            (fgate_cx): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (igate_cgate): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (fgate_cx_igate_cgate): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (ogate_cy): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (nn_fc2): Linear(\n",
       "    in_features=128, out_features=3, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (up_sample): Upsample(size=(16, 24), mode='bilinear')\n",
       "  (pxShuffle): PixelShuffle(upscale_factor=2)\n",
       "  (down_sample): Conv2d(\n",
       "    48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (concat): FloatFunctional(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QAT configuration\n",
    "quant_ita_lstm.train()\n",
    "quant_ita_lstm.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
    "quant_ita_lstm.fuse_model()  \n",
    "torch.quantization.prepare_qat(quant_ita_lstm, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2335823",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_ita_out = quant_ita_lstm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b69ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "print(quant_ita_out[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c9d23",
   "metadata": {},
   "source": [
    "Then we need to generate a dummy input for a single forward pass that normalises the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generate_dummy_input()\n",
    "print(f\"Input shape: {[x.shape for x in X]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_dummy_input()\n",
    "# Perform a forward pass to normalize the weights\n",
    "quantized_lstmnetvit.eval()\n",
    "with torch.no_grad():\n",
    "    x1 = quantized_lstmnetvit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade14420",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Output shape: {x1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x1))\n",
    "\n",
    "print(x[1].shape)\n",
    "print(x[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce614bd7",
   "metadata": {},
   "source": [
    "Now we can remove the spectral norm wrappers and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.remove_spectral_norm(model.decoder)  \n",
    "torch.nn.utils.remove_spectral_norm(model.nn_fc2)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/pretrained_models/checkpoints_for_qat/ITALSTM.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1021091",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnetvit = LSTMNetVIT()\n",
    "italstm = ITALSTM()\n",
    "quantized_lstmnetvit = QuantReadyLSTMNetViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnetvit.eval()\n",
    "with torch.no_grad():\n",
    "    x1 = lstmnetvit(x)\n",
    "print(f\"Output shape: {x1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309201d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66709e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_lstmnetvit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pretrained_weights = torch.load(\"../models/pretrained_models/checkpoints_for_qat/ITALSTM.pth\", map_location='cpu')\n",
    "# Load the pretrained weights\n",
    "quantized_lstmnetvit.load_state_dict(pretrained_weights, strict=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubflight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
