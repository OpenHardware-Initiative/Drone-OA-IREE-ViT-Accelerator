{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05fd3f4",
   "metadata": {},
   "source": [
    "# Heterogeneous Mini‑Models – Export & Quantize to ONNX\n",
    "\n",
    "This standalone notebook builds **four neural‑network toy models**, each illustrating a different\n",
    "mix of operators (Conv, custom Mix‑Transformer attention, LSTM, Dense). It then\n",
    "exports every network to **ONNX opset 17** and produces an **INT8 weight‑only** version\n",
    "using ONNX Runtime’s dynamic quantizer.\n",
    "\n",
    "To make the code easily hackable, *all logic lives in this notebook* – no extra\n",
    "files needed. GPU is **not** required; CPU is fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1248d6b",
   "metadata": {},
   "source": [
    "## 0. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch==2.2 onnx onnxruntime onnxruntime-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d9436",
   "metadata": {},
   "source": [
    "## 1. Mix‑Transformer building blocks  \n",
    "The custom encoder layer below follows the *MiT* paper (\"Mix Transformer: A Hybrid Strategy for Vision Transformers\").  \n",
    "Four helper classes work together:\n",
    "\n",
    "1. **`OverlapPatchMerging`** – slides a convolution with stride > 1 to\n",
    "   turn the input image into overlapping patches **and** reduce the spatial\n",
    "   resolution. The conv’s output `(B,C,H',W')` is flattened into\n",
    "   `(B, N, C)` where `N = H'·W'`, then LayerNorm is applied along the channel\n",
    "   dimension.\n",
    "2. **`EfficientSelfAttention`** – two‑stage self‑attention:\n",
    "   * *Reduction stage* – a strided conv downsamples keys & values so the\n",
    " \tquadratic `N×N` cost is lowered.\n",
    "   * *Attention stage* – queries come from the **original** tokens, while\n",
    " \tkeys/values come from the reduced set. A classic scaled dot‑product\n",
    " \twith softmax is followed by a final projection.\n",
    "3. **`MixFFN`** – feed‑forward network that first expands channels with an\n",
    "   MLP, applies a **depth‑wise 3×3 conv** (injecting local inductive bias),\n",
    "   then projects back to the original width.\n",
    "4. **`MixTransformerEncoderLayer`** – wraps everything: patch merging once,\n",
    "   followed by *N* residual **(Attention → MixFFN → LayerNorm)** blocks.\n",
    "\n",
    "The implementation matches the code you provided; extra inline comments were\n",
    "added for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af29ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------- 1. Overlapping patch embedding ----------\n",
    "class OverlapPatchMerging(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, patch_size, stride, padding):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# Conv extracts overlapped patches and does down‑sampling.\n",
    "\t\tself.cn1 = nn.Conv2d(in_channels, out_channels,\n",
    "\t\t\t\t\t\t\tkernel_size=patch_size,\n",
    "\t\t\t\t\t\t\tstride=stride, padding=padding)\n",
    "\t\tself.layerNorm = nn.LayerNorm(out_channels)\n",
    "\n",
    "\tdef forward(self, patches):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tpatches: (B, C, H, W)\n",
    "\t\tReturns:\n",
    "\t\t\tx : (B, N, C_out)   flattened sequence\n",
    "\t\t\tH', W' : new spatial dims\n",
    "\t\t\"\"\"\n",
    "\t\tx = self.cn1(patches)       \t# (B,C_out,H',W')\n",
    "\t\t_,_,H, W = x.shape\n",
    "\t\tx = x.flatten(2).transpose(1,2) # (B, N, C_out) where N = H'*W'\n",
    "\t\tx = self.layerNorm(x)\n",
    "\t\treturn x, H, W\n",
    "\n",
    "# ---------- 2. Token‑reduced self‑attention ----------\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "\tdef __init__(self, channels, reduction_ratio, num_heads):\n",
    "\t\tsuper().__init__()\n",
    "\t\tassert channels % num_heads == 0, \"channels must divide num_heads\"\n",
    "\t\tself.heads = num_heads\n",
    "\n",
    "\t\t# Reduction: stride = reduction_ratio\n",
    "\t\tself.cn1 = nn.Conv2d(channels, channels,\n",
    "\t\t\t\t\t\t\tkernel_size=reduction_ratio,\n",
    "\t\t\t\t\t\t\tstride=reduction_ratio)\n",
    "\t\tself.ln1 = nn.LayerNorm(channels)\n",
    "\n",
    "\t\t# Attention projections\n",
    "\t\tself.keyValueExtractor = nn.Linear(channels, channels * 2)\n",
    "\t\tself.query         \t= nn.Linear(channels, channels)\n",
    "\n",
    "\t\tself.smax   \t= nn.Softmax(dim=-1)\n",
    "\t\tself.finalLayer = nn.Linear(channels, channels)\n",
    "\n",
    "\tdef forward(self, x, H, W):\n",
    "\t\t\"\"\"\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tx : (B, N, C) with N = H*W\n",
    "\t\t\t\tH, W : spatial size before flattening\n",
    "\t\t\tReturns:\n",
    "\t\t\t\t(B, N, C)\n",
    "\t\t\"\"\"\n",
    "\t\tB,N,C = x.shape\n",
    "\n",
    "\t\t# 1. reduce tokens for K,V ------------------------------------------\n",
    "\t\tx1 = x.permute(0,2,1).reshape(B,C,H,W)  # (B,C,H,W)\n",
    "\t\tx1 = self.cn1(x1)                   \t# (B,C,H/rr,W/rr)\n",
    "\t\tx1 = x1.reshape(B,C,-1).permute(0,2,1).contiguous() # (B,N',C)\n",
    "\t\tx1 = self.ln1(x1)\n",
    "\n",
    "\t\t# 2. project to Q,K,V -----------------------------------------------\n",
    "\t\tkv = self.keyValueExtractor(x1)     \t# (B,N',2C)\n",
    "\t\tkv = kv.reshape(B,-1,2,self.heads,C//self.heads)\n",
    "\t\tkv = kv.permute(2,0,3,1,4)          \t# (2,B,h,N',c/h)\n",
    "\t\tk, v = kv[0], kv[1]\n",
    "\n",
    "\t\tq = self.query(x).reshape(B,N,self.heads,C//self.heads)\n",
    "\t\tq = q.permute(0,2,1,3)              \t# (B,h,N,c/h)\n",
    "\n",
    "\t\t# 3. scaled dot‑product attention -----------------------------------\n",
    "\t\tdim_head = (C/self.heads) ** 0.5\n",
    "\t\tattn = self.smax(q @ k.transpose(-2,-1) / dim_head)  # (B,h,N,N')\n",
    "\t\tctx  = (attn @ v).transpose(1,2).reshape(B,N,C)  \t# (B,N,C)\n",
    "\n",
    "\t\treturn self.finalLayer(ctx)         \t# (B,N,C)\n",
    "\n",
    "# ---------- 3. Feed‑forward network with depth‑wise conv ----------\n",
    "class MixFFN(nn.Module):\n",
    "\tdef __init__(self, channels, expansion_factor):\n",
    "\t\tsuper().__init__()\n",
    "\t\texp = channels * expansion_factor\n",
    "\t\tself.mlp1 = nn.Linear(channels, exp)\n",
    "\t\tself.depthwise = nn.Conv2d(exp, exp, 3,\n",
    "\t\t\t\t\t\t\t\tpadding=1, groups=channels)\n",
    "\t\tself.gelu = nn.GELU()\n",
    "\t\tself.mlp2 = nn.Linear(exp, channels)\n",
    "\n",
    "\tdef forward(self, x, H, W):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tx : (B, N, C)\n",
    "\t\t\"\"\"\n",
    "\t\tx = self.mlp1(x)                    \t# (B,N,exp)\n",
    "\t\tB,N,C = x.shape\n",
    "\t\tx = x.transpose(1,2).view(B,C,H,W)  \t# (B,exp,H,W)\n",
    "\t\tx = self.gelu(self.depthwise(x).flatten(2).transpose(1,2))\n",
    "\t\treturn self.mlp2(x)                 \t# (B,N,C)\n",
    "\n",
    "# ---------- 4. Full encoder layer ------------------------------------------\n",
    "class MixTransformerEncoderLayer(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, patch_size, stride, padding,\n",
    "             \tn_layers, reduction_ratio, num_heads, expansion_factor):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.patchMerge = OverlapPatchMerging(in_channels, out_channels,\n",
    "\t\t\t\t\t\t\t\t\t\t\tpatch_size, stride, padding)\n",
    "\t\tself._attn  = nn.ModuleList(\n",
    "\t\t\t[EfficientSelfAttention(out_channels, reduction_ratio, num_heads)\n",
    "\t\t\tfor _ in range(n_layers)])\n",
    "\t\tself._ffn   = nn.ModuleList(\n",
    "\t\t\t[MixFFN(out_channels, expansion_factor) for _ in range(n_layers)])\n",
    "\t\tself._lNorm = nn.ModuleList(\n",
    "\t\t\t[nn.LayerNorm(out_channels) for _ in range(n_layers)])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tx : (B, C_in, H, W)\n",
    "\t\tReturns:\n",
    "\t\t\t(B, C_out, H', W')\n",
    "\t\t\"\"\"\n",
    "\t\tB,C,H,W = x.shape\n",
    "\t\tx, H, W = self.patchMerge(x)        \t# (B,N,C_out)\n",
    "\t\tfor attn, ffn, ln in zip(self._attn, self._ffn, self._lNorm):\n",
    "\t\t\tx = ln(x + attn(x, H, W))       \t# residual + norm\n",
    "\t\t\tx = ln(x + ffn(x, H, W))\n",
    "\t\tx = x.reshape(B, H, W, -1).permute(0,3,1,2).contiguous()\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99926236",
   "metadata": {},
   "source": [
    "## 2. Four toy networks\n",
    "Below we compose the encoder layer into four miniature models of increasing\n",
    "complexity.\n",
    "\n",
    "| Model | Branch‑1 | Branch‑2 | Recurrent | Head |\n",
    "|-------|----------|----------|-----------|------|\n",
    "| `HeteroVIT`   | Conv | **Mix‑Transformer** | LSTM | Dense |\n",
    "| `HeteroDense` | Conv | Dense | LSTM | Dense |\n",
    "| `CNN2_LSTM`   | Conv×2 | — | LSTM | Dense |\n",
    "| `CNN1_Dense`  | Conv | — | — | Dense |\n",
    "\n",
    "All share the same input shape **(B, 1, 60, 90)** to keep ONNX export simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc9465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------- variant A -------------------------------------------------------\n",
    "class HeteroVIT(nn.Module):\n",
    "\tdef __init__(self, lstm_h=64, n_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv = nn.Conv2d(1, 8, 3, padding=1)\n",
    "\t\tself.vit  = MixTransformerEncoderLayer(1, 32, 4, 4, 0,\n",
    "                                           \tn_layers=1,\n",
    "                                           \treduction_ratio=4,\n",
    "                                           \tnum_heads=4,\n",
    "                                           \texpansion_factor=4)\n",
    "\t\tself.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\t\tself.lstm = nn.LSTM(8*60*90 + 32, lstm_h, batch_first=True)\n",
    "\t\tself.fc   = nn.Linear(lstm_h, n_classes)\n",
    "\n",
    "\tdef forward(self, x, h0=None):\n",
    "\t\ta = self.conv(x).flatten(1)\n",
    "\t\tp = self.pool(self.vit(x)).flatten(1)\n",
    "\t\ty,(h,c) = self.lstm(torch.cat([a,p],1).unsqueeze(1), h0)\n",
    "\t\treturn self.fc(y.squeeze(1)), h, c\n",
    "\n",
    "# ---------- variant B -------------------------------------------------------\n",
    "class HeteroDense(nn.Module):\n",
    "\tdef __init__(self, lstm_h=64, n_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv = nn.Conv2d(1, 8, 3, padding=1)\n",
    "\t\tself.proj = nn.Linear(60*90, 32)\n",
    "\t\tself.lstm = nn.LSTM(8*60*90 + 32, lstm_h, batch_first=True)\n",
    "\t\tself.fc   = nn.Linear(lstm_h, n_classes)\n",
    "\n",
    "\tdef forward(self, x, h0=None):\n",
    "\t\ta = self.conv(x).flatten(1)\n",
    "\t\tp = self.proj(x.flatten(1))\n",
    "\t\ty,(h,c) = self.lstm(torch.cat([a,p],1).unsqueeze(1), h0)\n",
    "\t\treturn self.fc(y.squeeze(1)), h, c\n",
    "\n",
    "# ---------- variant C -------------------------------------------------------\n",
    "class CNN2_LSTM(nn.Module):\n",
    "\tdef __init__(self, lstm_h=64, n_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv1 = nn.Conv2d(1,8,3,padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(8,16,3,padding=1)\n",
    "\t\tself.lstm  = nn.LSTM(16*60*90, lstm_h, batch_first=True)\n",
    "\t\tself.fc\t= nn.Linear(lstm_h, n_classes)\n",
    "\n",
    "\tdef forward(self, x, h0=None):\n",
    "\t\tf = self.conv2(self.conv1(x)).flatten(1)\n",
    "\t\ty,(h,c) = self.lstm(f.unsqueeze(1), h0)\n",
    "\t\treturn self.fc(y.squeeze(1)), h, c\n",
    "\n",
    "# ---------- variant D -------------------------------------------------------\n",
    "class CNN1_Dense(nn.Module):\n",
    "\tdef __init__(self, n_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv = nn.Conv2d(1,8,3,padding=1)\n",
    "\t\tself.fc   = nn.Linear(8*60*90, n_classes)\n",
    "\n",
    "\tdef forward(self, x, *args):\n",
    "\t\treturn self.fc(self.conv(x).flatten(1)), torch.Tensor(1), torch.Tensor(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e60cc",
   "metadata": {},
   "source": [
    "## 3. Export to ONNX & dynamic INT8 quantization\n",
    " * ONNX **opset 17** keeps the LSTM and custom attention intact.\n",
    " * **Dynamic** quantization converts weights to INT8 (`QInt8`), leaving\n",
    "   activations in FP32 – no calibration data needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b61ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agustin_nahuel/miniconda3/envs/mobileSAM/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved models/dummy/hetero_vit.onnx\n",
      "saved models/dummy/hetero_vit_int8.onnx\n",
      "output shape torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved models/dummy/hetero_dense.onnx\n",
      "saved models/dummy/hetero_dense_int8.onnx\n",
      "output shape torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved models/dummy/cnn2_lstm.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_907869/3087768815.py:61: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return self.fc(self.conv(x).flatten(1)), torch.Tensor(1), torch.Tensor(1)\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved models/dummy/cnn2_lstm_int8.onnx\n",
      "output shape torch.Size([1, 10])\n",
      "saved models/dummy/cnn1_dense.onnx\n",
      "saved models/dummy/cnn1_dense_int8.onnx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import torch\n",
    "\n",
    "MODELS = {\n",
    "\t\"hetero_vit\"  : HeteroVIT(),\n",
    "\t\"hetero_dense\": HeteroDense(),\n",
    "\t\"cnn2_lstm\"   : CNN2_LSTM(),\n",
    "\t\"cnn1_dense\"  : CNN1_Dense(),\n",
    "}\n",
    "\n",
    "out_dir = Path(\"models/dummy\"); out_dir.mkdir(exist_ok=True)\n",
    "example = torch.randn(1,1,60,90)\n",
    "opset   = 17\n",
    "\n",
    "for name, net in MODELS.items():\n",
    "\tfp32 = out_dir / f\"{name}.onnx\"\n",
    "\tint8 = out_dir / f\"{name}_int8.onnx\"\n",
    "\n",
    "\ttry:\n",
    "\t\toutput_example = net(example)\n",
    "\t\tprint(\"output shape\", output_example[0].shape)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"model forward failed\", e)\n",
    "\t\tcontinue\n",
    "\n",
    "\t# Export the model to ONNX format\n",
    "\ttry:\n",
    "\t\ttorch.onnx.export(net.eval(), example, fp32,\n",
    "\t\t\t\t\t\topset_version=opset,\n",
    "\t\t\t\t\t\tinput_names=[\"input\"],\n",
    "\t\t\t\t\t\toutput_names=[\"logits\",\"h\",\"c\"],\n",
    "\t\t\t\t\t\tdo_constant_folding=True)\n",
    "\t\tprint(\"saved\", fp32)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"onnx export failed\", e)\n",
    "\t\tcontinue\n",
    "\n",
    "\t# Quantize the model\n",
    "\ttry:\n",
    "\t\tquantize_dynamic(fp32, int8,\n",
    "\t\t\t\t\t\tweight_type=QuantType.QInt8)\n",
    "\t\tprint(\"saved\", int8)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"onnx quantization failed\", e)\n",
    "\t\tcontinue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobileSAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
