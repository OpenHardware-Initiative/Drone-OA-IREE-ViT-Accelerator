{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported from ViTsubmodules.py and model.py.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "from third_party.vitfly.models.ViTsubmodules import *\n",
    "from third_party.vitfly.models.model import *\n",
    "\n",
    "print(\"Successfully imported from ViTsubmodules.py and model.py.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quantization Specific Wrapper ---\n",
    "class QuantizableLSTMNetVIT(LSTMNetVIT):\n",
    "    \"\"\"\n",
    "    A wrapper class that prepares the LSTMNetVIT model for static quantization.\n",
    "    It adds Quant/DeQuant stubs and handles the model's control flow for ONNX export.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Unpack the input tuple and apply the QuantStub to the image tensor\n",
    "        img, des_vel, quat, hidden_state = X\n",
    "        img = self.quant(img)\n",
    "\n",
    "        # Execute the original model's logic, but follow a single, traceable path\n",
    "        # by assuming the hidden state is always provided.\n",
    "        embeds = [img]\n",
    "        for block in self.encoder_blocks:\n",
    "            embeds.append(block(embeds[-1]))\n",
    "        \n",
    "        out = embeds[1:]\n",
    "        out = torch.cat([self.pxShuffle(out[1]), self.up_sample(out[0])], dim=1)\n",
    "        out = self.down_sample(out)\n",
    "        out = self.decoder(out.flatten(1))\n",
    "\n",
    "        out = torch.cat([out, des_vel / 10, quat], dim=1).float()\n",
    "        \n",
    "        # Explicitly use the execution path with a hidden state for ONNX compatibility\n",
    "        out, h = self.lstm(out, hidden_state)\n",
    "        \n",
    "        out = self.nn_fc2(out)\n",
    "\n",
    "        # Apply the DeQuantStub before returning the final output\n",
    "        out = self.dequant(out)\n",
    "        \n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def get_dummy_input(batch_size=1):\n",
    "    \"\"\"Generates a valid dummy input tuple for the model.\"\"\"\n",
    "    image = torch.randn(batch_size, 1, 60, 90)\n",
    "    des_vel = torch.randn(batch_size, 3)\n",
    "    quat = torch.randn(batch_size, 2)\n",
    "    # LSTM hidden state: (num_layers, batch_size, hidden_size)\n",
    "    hidden = (torch.randn(3, batch_size, 128), torch.randn(3, batch_size, 128))\n",
    "    return (image, des_vel, quat, hidden)\n",
    "\n",
    "def print_model_size(model, label):\n",
    "    \"\"\"Prints the size of a model's state_dict in MB.\"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_mb = os.path.getsize(\"temp.p\") / 1.e6\n",
    "    print(f\"{label}: {size_mb:.2f} MB\")\n",
    "    os.remove(\"temp.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Step 1: Preparing the model...\n",
      "✅ Model prepared.\n",
      "Original FP32 model size: 14.28 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Prepare the Model ---\n",
    "print(\"🚀 Step 1: Preparing the model...\")\n",
    "# Instantiate the quantizable wrapper\n",
    "model_fp32 = QuantizableLSTMNetVIT()\n",
    "\n",
    "# IMPORTANT: Load your pre-trained weights here\n",
    "#model_fp32.load_state_dict(torch.load('/Projects/Drone-ViT-HW-Accelerator/weights/lstm_vit.pth'))\n",
    "\n",
    "# Remove spectral normalization, which interferes with quantization\n",
    "torch.nn.utils.remove_spectral_norm(model_fp32.decoder)\n",
    "torch.nn.utils.remove_spectral_norm(model_fp32.nn_fc2)\n",
    "\n",
    "model_fp32.eval()\n",
    "print(\"✅ Model prepared.\")\n",
    "print_model_size(model_fp32, \"Original FP32 model size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Step 2: Configuring quantization...\n",
      "✅ Quantization configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Configure Quantization ---\n",
    "print(\"\\n⚙️ Step 2: Configuring quantization...\")\n",
    "# Use 'fbgemm' for x86 CPUs. Use 'qnnpack' for ARM.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Fuse modules for better performance (Conv-BN-ReLU, etc.)\n",
    "# Note: Your model doesn't have obvious patterns like Conv-BN, but this is best practice.\n",
    "#model_fp32_fused = torch.quantization.fuse_modules(model_fp32, []) # No modules to fuse in this case\n",
    "\n",
    "# Insert observers to collect activation statistics\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32)\n",
    "print(\"✅ Quantization configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 Step 3: Calibrating the model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 1, 517, 128]' is invalid for input of size 384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      6\u001b[0m         dummy_input \u001b[38;5;241m=\u001b[39m get_dummy_input()\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mmodel_fp32_prepared\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Calibration complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileSAM/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileSAM/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 31\u001b[0m, in \u001b[0;36mQuantizableLSTMNetVIT.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([out, des_vel \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m, quat], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Explicitly use the execution path with a hidden state for ONNX compatibility\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m out, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_fc2(out)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Apply the DeQuantStub before returning the final output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileSAM/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileSAM/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileSAM/lib/python3.9/site-packages/torch/ao/nn/quantizable/modules/rnn.py:524\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m    522\u001b[0m hidden_non_opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unwrap_optional(hidden)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hidden_non_opt[\u001b[38;5;241m0\u001b[39m], Tensor):\n\u001b[0;32m--> 524\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_non_opt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_directions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     cx \u001b[38;5;241m=\u001b[39m hidden_non_opt[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, num_directions, max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m     hxcx \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    531\u001b[0m         (hx[idx]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), cx[idx]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers)\n\u001b[1;32m    533\u001b[0m     ]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 1, 517, 128]' is invalid for input of size 384"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Calibrate the Model ---\n",
    "print(\"\\n🔬 Step 3: Calibrating the model...\")\n",
    "# Pass representative data through the model. For this example, we use dummy data.\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        dummy_input = get_dummy_input()\n",
    "        model_fp32_prepared(dummy_input)\n",
    "print(\"✅ Calibration complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobileSAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
