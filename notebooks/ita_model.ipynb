{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agustin_nahuel/.local/lib/python3.10/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/agustin_nahuel/.local/lib/python3.10/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "import iree.turbine.aot as aot\n",
    "import numpy as np\n",
    "import iree.runtime as ireert\n",
    "\n",
    "# --- Helper Functions and Placeholders ---\n",
    "\n",
    "def refine_inputs(X):\n",
    "    \"\"\"\n",
    "    Placeholder for user's input refinement function.\n",
    "    Replace this with your actual implementation.\n",
    "    \"\"\"\n",
    "    return X\n",
    "\n",
    "def ita_partial_max(logits: torch.Tensor, k: int = 8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Emulates ITAPartialMax by applying softmax to only the top-k elements.\n",
    "    \"\"\"\n",
    "    seq_len = logits.size(-1)\n",
    "    k = min(k, seq_len)\n",
    "    topk_vals, topk_indices = torch.topk(logits, k, dim=-1)\n",
    "    mask = torch.zeros_like(logits).scatter(-1, topk_indices, 1.0)\n",
    "    masked_logits = logits * mask\n",
    "    weights = F.softmax(masked_logits, dim=-1)\n",
    "    return weights\n",
    "\n",
    "# --- Model Definitions ---\n",
    "\n",
    "class OverlapPatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(in_channels, out_channels, kernel_size=patch_size, stride=stride, padding=padding)\n",
    "        self.layerNorm = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        x = self.cn1(patches)\n",
    "        _, _, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.layerNorm(x)\n",
    "        return x, H, W\n",
    "\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio, num_heads):\n",
    "        super().__init__()\n",
    "        assert channels % num_heads == 0, f\"channels {channels} should be divided by num_heads {num_heads}.\"\n",
    "        self.heads = num_heads\n",
    "        self.cn1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=reduction_ratio, stride=reduction_ratio)\n",
    "        self.ln1 = nn.LayerNorm(channels)\n",
    "        self.keyValueExtractor = nn.Linear(channels, channels * 2)\n",
    "        self.query = nn.Linear(channels, channels)\n",
    "        self.smax = nn.Softmax(dim=-1)\n",
    "        self.finalLayer = nn.Linear(channels, channels)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x1 = x.clone().permute(0, 2, 1)\n",
    "        x1 = x1.reshape(B, C, H, W)\n",
    "        x1 = self.cn1(x1)\n",
    "        x1 = x1.reshape(B, C, -1).permute(0, 2, 1).contiguous()\n",
    "        x1 = self.ln1(x1)\n",
    "        keyVal = self.keyValueExtractor(x1)\n",
    "        keyVal = keyVal.reshape(B, -1, 2, self.heads, int(C / self.heads)).permute(2, 0, 3, 1, 4).contiguous()\n",
    "        k, v = keyVal[0], keyVal[1]\n",
    "        q = self.query(x).reshape(B, N, self.heads, int(C / self.heads)).permute(0, 2, 1, 3).contiguous()\n",
    "        dimHead = (C / self.heads)**0.5\n",
    "        attention = self.smax(q @ k.transpose(-2, -1) / dimHead)\n",
    "        attention = (attention @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.finalLayer(attention)\n",
    "        return x\n",
    "\n",
    "class MixFFN(nn.Module):\n",
    "    def __init__(self, channels, expansion_factor):\n",
    "        super().__init__()\n",
    "        self.expanded_channels = channels * expansion_factor\n",
    "        self.mlp1 = nn.Linear(channels, self.expanded_channels)\n",
    "        self.depthwise = nn.Conv2d(self.expanded_channels, self.expanded_channels, kernel_size=3, padding='same', groups=self.expanded_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.mlp2 = nn.Linear(self.expanded_channels, channels)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = self.mlp1(x)\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, self.expanded_channels, H, W)\n",
    "        x = self.gelu(self.depthwise(x).flatten(2).transpose(1, 2))\n",
    "        x = self.mlp2(x)\n",
    "        return x\n",
    "\n",
    "class MixTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, stride, padding,\n",
    "                 n_layers, reduction_ratio, num_heads, expansion_factor):\n",
    "        super().__init__()\n",
    "        self.patchMerge = OverlapPatchMerging(in_channels, out_channels, patch_size, stride, padding)\n",
    "        self._attn = nn.ModuleList([EfficientSelfAttention(out_channels, reduction_ratio, num_heads) for _ in range(n_layers)])\n",
    "        self._ffn = nn.ModuleList([MixFFN(out_channels, expansion_factor) for _ in range(n_layers)])\n",
    "        self._lNorm = nn.ModuleList([nn.LayerNorm(out_channels) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x, H, W = self.patchMerge(x)\n",
    "        for i in range(len(self._attn)):\n",
    "            x = x + self._attn[i].forward(x, H, W)\n",
    "            x = x + self._ffn[i].forward(x, H, W)\n",
    "            x = self._lNorm[i].forward(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        return x\n",
    "\n",
    "class MultiheadITAWithRequant(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, params=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        assert params is not None, \"Parameters for requantization must be provided\"\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.params = params\n",
    "\n",
    "    def requant_shift(self, x, mult, shift):\n",
    "        x = x * mult\n",
    "        x = torch.div(x, 2**shift, rounding_mode='floor')\n",
    "        return torch.clamp(x + self.params[\"zp\"], -128, 127).to(torch.int8)\n",
    "\n",
    "    def forward(self, q_input, kv_input):\n",
    "        B_q, N_q, _ = q_input.shape\n",
    "        B_kv, N_kv, _ = kv_input.shape\n",
    "        Q = self.q_proj(q_input).reshape(B_q, N_q, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(kv_input).reshape(B_kv, N_kv, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(kv_input).reshape(B_kv, N_kv, self.num_heads, self.head_dim)\n",
    "        Q = self.requant_shift(Q.to(torch.int32), self.params[\"mq\"], self.params[\"sq\"])\n",
    "        K = self.requant_shift(K.to(torch.int32), self.params[\"mk\"], self.params[\"sk\"])\n",
    "        V = self.requant_shift(V.to(torch.int32), self.params[\"mv\"], self.params[\"sv\"])\n",
    "        Q = Q.permute(0, 2, 1, 3).to(torch.float32)\n",
    "        K = K.permute(0, 2, 1, 3).to(torch.float32)\n",
    "        V = V.permute(0, 2, 1, 3)\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attn_logits = self.requant_shift(attn_logits, self.params[\"ma\"], self.params[\"sa\"])\n",
    "        attn_weights = ita_partial_max(attn_logits.float(), k=8)\n",
    "        context = torch.matmul(attn_weights, V.to(torch.float32))\n",
    "        context = self.requant_shift(context, self.params[\"mav\"], self.params[\"sav\"])\n",
    "        context = context.permute(0, 2, 1, 3).reshape(B_q, N_q, self.embed_dim)\n",
    "        output = self.out_proj(context.to(torch.float32))\n",
    "        output = self.requant_shift(output.to(torch.int32), self.params[\"mo\"], self.params[\"so\"])\n",
    "        final = self.requant_shift(output.to(torch.int32), self.params[\"mf\"], self.params[\"sf\"])\n",
    "        return final\n",
    "\n",
    "class ITASelfAttentionWrapper(nn.Module):\n",
    "    def __init__(self, channels, embed_dim, num_heads, reduction_ratio, efficient_attn, itaparameters):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=reduction_ratio, stride=reduction_ratio)\n",
    "        self.ln1 = nn.LayerNorm(channels)\n",
    "        self.self_attn = MultiheadITAWithRequant(embed_dim=embed_dim, num_heads=num_heads, params=itaparameters)\n",
    "        self.efficient_attn = efficient_attn\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        if self.efficient_attn:\n",
    "            x1 = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "            x1 = self.cn1(x1)\n",
    "            x1 = x1.reshape(B, C, -1).permute(0, 2, 1).contiguous()\n",
    "            x1 = self.ln1(x1)\n",
    "            out = self.self_attn(x, x1)\n",
    "        else:\n",
    "            out = self.self_attn(x, x)\n",
    "        return out.float()\n",
    "\n",
    "class MiXITAEncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, stride, padding,\n",
    "                 n_layers, reduction_ratio, num_heads, expansion_factor, embed_dim, efficient_attn=True, itaparameters=None):\n",
    "        super().__init__()\n",
    "        self.patchMerge = OverlapPatchMerging(in_channels, out_channels, patch_size, stride, padding)\n",
    "        self._attn = nn.ModuleList([ITASelfAttentionWrapper(channels=out_channels, embed_dim=embed_dim, num_heads=num_heads,\n",
    "                                                            reduction_ratio=reduction_ratio, efficient_attn=efficient_attn,\n",
    "                                                            itaparameters=itaparameters) for _ in range(n_layers)])\n",
    "        self._ffn = nn.ModuleList([MixFFN(out_channels, expansion_factor) for _ in range(n_layers)])\n",
    "        self._lNorms = nn.ModuleList([nn.LayerNorm(out_channels) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x, H, W = self.patchMerge(x)\n",
    "        for i in range(len(self._attn)):\n",
    "            x = x + self._attn[i].forward(x, H, W)\n",
    "            x = x + self._ffn[i].forward(x, H, W)\n",
    "            x = self._lNorms[i].forward(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        return x\n",
    "\n",
    "class ITALSTM(nn.Module):\n",
    "    def __init__(self, itaparameters=None, efficient_attn=True):\n",
    "        super().__init__()\n",
    "        if itaparameters is None:\n",
    "            itaparameters = {\"mq\": 1.0, \"sq\": 0, \"mk\": 1.0, \"sk\": 0, \"mv\": 1.0, \"sv\": 0,\n",
    "                             \"ma\": 1.0, \"sa\": 0, \"mav\": 1.0, \"sav\": 0, \"mo\": 1.0, \"so\": 0,\n",
    "                             \"mf\": 1.0, \"sf\": 0, \"zp\": 0}\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            MiXITAEncoderLayer(1, 32, 7, 4, 3, 2, 8, 1, 8, 32, efficient_attn, itaparameters),\n",
    "            MiXITAEncoderLayer(32, 64, 3, 2, 1, 2, 4, 2, 8, 64, efficient_attn, itaparameters)])\n",
    "        self.decoder = spectral_norm(nn.Linear(4608, 512))\n",
    "        self.lstm = nn.LSTM(input_size=517, hidden_size=128, num_layers=3, dropout=0.1)\n",
    "        self.nn_fc2 = spectral_norm(nn.Linear(128, 3))\n",
    "        self.up_sample = nn.Upsample(size=(16, 24), mode='bilinear', align_corners=True)\n",
    "        self.pxShuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "        self.down_sample = nn.Conv2d(48, 12, 3, padding=1)\n",
    "\n",
    "    def _encode(self, x):\n",
    "        embeds = [x]\n",
    "        for block in self.encoder_blocks:\n",
    "            embeds.append(block(embeds[-1]))\n",
    "        return embeds[1:]\n",
    "\n",
    "    def _decode(self, encoded_features):\n",
    "        out = torch.cat([self.pxShuffle(encoded_features[1]), self.up_sample(encoded_features[0])], dim=1)\n",
    "        out = self.down_sample(out)\n",
    "        return self.decoder(out.flatten(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = refine_inputs(X)\n",
    "        x = X[0]\n",
    "        encoded_features = self._encode(x)\n",
    "        out = self._decode(encoded_features)\n",
    "        out = torch.cat([out, X[1].squeeze(0) / 10, X[2].squeeze(0)], dim=1).float()\n",
    "        if len(X) > 3:\n",
    "            out, h = self.lstm(out, X[3])\n",
    "        else:\n",
    "            out, h = self.lstm(out)\n",
    "        out = self.nn_fc2(out)\n",
    "        return out, h\n",
    "\n",
    "# --- Dummy Input Generation ---\n",
    "def generate_dummy_input(traj_len=10):\n",
    "    depth_images = torch.randn(traj_len, 1, 60, 90)\n",
    "    control_input = torch.rand(traj_len, 1)\n",
    "    orientation = torch.tensor([[1.0, 0.0, 0.0, 0.0]] * traj_len).float()\n",
    "    return [depth_images, control_input, orientation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate Model and Dummies ---\n",
    "torch.manual_seed(0)\n",
    "ita_lstm_module = ITALSTM()\n",
    "ita_lstm_module.eval()\n",
    "\n",
    "# Generate dummy inputs to get shapes for abstract tensors\n",
    "dummy_input_initial = generate_dummy_input(traj_len=10)\n",
    "with torch.no_grad():\n",
    "    _, (dummy_h_n, dummy_c_n) = ita_lstm_module(dummy_input_initial)\n",
    "\n",
    "# --- Define the Compiled Module API for IREE ---\n",
    "\n",
    "class CompiledITALSTM(aot.CompiledModule):\n",
    "    # Export all parameters from the PyTorch module.\n",
    "    # 'mutable=True' allows weights to be updated in the compiled artifact.\n",
    "    params = aot.export_parameters(ita_lstm_module, mutable=True)\n",
    "    \n",
    "    # Make the forward method of the model jittable.\n",
    "    compute = aot.jittable(ita_lstm_module.forward)\n",
    "\n",
    "    def main(self,\n",
    "             depth_images=aot.abstractify(dummy_input_initial[0]),\n",
    "             control_input=aot.abstractify(dummy_input_initial[1]),\n",
    "             orientation=aot.abstractify(dummy_input_initial[2])):\n",
    "        \"\"\"\n",
    "        Exported function for the initial run (no hidden state).\n",
    "        The model's forward pass expects a list of tensors.\n",
    "        \"\"\"\n",
    "        return self.compute([depth_images, control_input, orientation])\n",
    "\n",
    "    def run_with_state(self,\n",
    "                       depth_images=aot.abstractify(dummy_input_initial[0]),\n",
    "                       control_input=aot.abstractify(dummy_input_initial[1]),\n",
    "                       orientation=aot.abstractify(dummy_input_initial[2]),\n",
    "                       h_n=aot.abstractify(dummy_h_n),\n",
    "                       c_n=aot.abstractify(dummy_c_n)):\n",
    "        \"\"\"\n",
    "        Exported function for recurrent runs, including the LSTM state.\n",
    "        The hidden state tuple (h_n, c_n) is reconstructed from the inputs.\n",
    "        \"\"\"\n",
    "        hidden_state = (h_n, c_n)\n",
    "        return self.compute([depth_images, control_input, orientation, hidden_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model using the AOT toolkit\n",
    "export_output = aot.export(CompiledITALSTM)\n",
    "\n",
    "# Compile the exported model to a binary artifact in memory\n",
    "compiled_binary = export_output.compile(save_to=None)\n",
    "\n",
    "# --- Use the IREE runtime to test the compiled program ---\n",
    "config = ireert.Config(\"local-task\")\n",
    "vm_module = ireert.load_vm_module(\n",
    "    ireert.VmModule.wrap_buffer(config.vm_instance, compiled_binary.map_memory()),\n",
    "    config,\n",
    ")\n",
    "\n",
    "# Prepare NumPy inputs from the dummy PyTorch tensors\n",
    "depth_np = dummy_input_initial[0].numpy()\n",
    "control_np = dummy_input_initial[1].numpy()\n",
    "orientation_np = dummy_input_initial[2].numpy()\n",
    "\n",
    "# Run the 'main' function (initial pass)\n",
    "result_out, (h_n, c_n) = vm_module.main(depth_np, control_np, orientation_np)\n",
    "\n",
    "print(\"--- Initial Run ('main') ---\")\n",
    "print(\"Output shape:\", result_out.to_host().shape)\n",
    "print(\"Hidden state 'h_n' shape:\", h_n.to_host().shape)\n",
    "print(\"Cell state 'c_n' shape:\", c_n.to_host().shape)\n",
    "\n",
    "# Run the 'run_with_state' function (recurrent pass)\n",
    "result_out_rec, (h_n_rec, c_n_rec) = vm_module.run_with_state(depth_np, control_np, orientation_np, h_n, c_n)\n",
    "\n",
    "print(\"\\n--- Recurrent Run ('run_with_state') ---\")\n",
    "print(\"Output shape:\", result_out_rec.to_host().shape)\n",
    "print(\"New hidden state 'h_n' shape:\", h_n_rec.to_host().shape)\n",
    "print(\"New cell state 'c_n' shape:\", c_n_rec.to_host().shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IREE_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
