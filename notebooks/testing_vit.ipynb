{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LSTM\n",
    "import torch.nn.utils.spectral_norm as spectral_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class OverlapPatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(in_channels, out_channels, kernel_size=patch_size, stride = stride, padding = padding)\n",
    "        self.layerNorm = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        \"\"\"Merge patches to reduce dimensions of input.\n",
    "\n",
    "        :param patches: tensor with shape (B, C, H, W) where\n",
    "            B is the Batch size\n",
    "            C is the number of Channels\n",
    "            H and W are the Height and Width\n",
    "        :return: tensor with shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        x = self.cn1(patches)\n",
    "        _,_,H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1,2) #Flatten - (B,C,H*W); transpose B,HW, C\n",
    "        x = self.layerNorm(x)\n",
    "        return x,H,W #B, N, EmbedDim\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio, num_heads):\n",
    "        super().__init__()\n",
    "        assert channels % num_heads == 0, f\"channels {channels} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.heads= num_heads\n",
    "\n",
    "        #### Self Attention Block consists of 2 parts - Reduction and then normal Attention equation of queries and keys###\n",
    "        \n",
    "        # Reduction Parameters #\n",
    "        self.cn1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=reduction_ratio, stride= reduction_ratio)\n",
    "        self.ln1 = nn.LayerNorm(channels)\n",
    "        # Attention Parameters #\n",
    "        self.keyValueExtractor = nn.Linear(channels, channels * 2)\n",
    "        self.query = nn.Linear(channels, channels)\n",
    "        self.smax = nn.Softmax(dim=-1)\n",
    "        self.finalLayer = nn.Linear(channels, channels) \n",
    "\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "\n",
    "        \"\"\" Perform self attention with reduced sequence length\n",
    "\n",
    "        :param x: tensor of shape (B, N, C) where\n",
    "            B is the batch size,\n",
    "            N is the number of queries (equal to H * W)\n",
    "            C is the number of channels\n",
    "        :return: tensor of shape (B, N, C)\n",
    "        \"\"\"\n",
    "        B,N,C = x.shape\n",
    "        # B, N, C -> B, C, N\n",
    "        x1 = x.clone().permute(0,2,1)\n",
    "        # BCN -> BCHW\n",
    "        x1 = x1.reshape(B,C,H,W)\n",
    "        x1 = self.cn1(x1)\n",
    "        x1 = x1.reshape(B,C,-1).permute(0,2,1).contiguous()\n",
    "        x1 = self.ln1(x1)\n",
    "        # We have got the Reduced Embeddings! We need to extract key and value pairs now\n",
    "        keyVal = self.keyValueExtractor(x1)\n",
    "        keyVal = keyVal.reshape(B, -1 , 2, self.heads, int(C/self.heads)).permute(2,0,3,1,4).contiguous()\n",
    "        k,v = keyVal[0],keyVal[1] #b,heads, n, c/heads\n",
    "        q = self.query(x).reshape(B, N, self.heads, int(C/self.heads)).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        dimHead = (C/self.heads)**0.5\n",
    "        attention = self.smax(q@k.transpose(-2, -1)/dimHead)\n",
    "        attention = (attention@v).transpose(1,2).reshape(B,N,C)\n",
    "\n",
    "        x = self.finalLayer(attention) #B,N,C        \n",
    "        return x\n",
    "\n",
    "class MixFFN(nn.Module):\n",
    "    def __init__(self, channels, expansion_factor):\n",
    "        super().__init__()\n",
    "        expanded_channels = channels*expansion_factor\n",
    "        #MLP Layer        \n",
    "        self.mlp1 = nn.Linear(channels, expanded_channels)\n",
    "        #Depth Wise CNN Layer\n",
    "        self.depthwise = nn.Conv2d(expanded_channels, expanded_channels, kernel_size=3,  padding='same', groups=channels)\n",
    "        #GELU\n",
    "        self.gelu = nn.GELU()\n",
    "        #MLP to predict\n",
    "        self.mlp2 = nn.Linear(expanded_channels, channels)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Perform self attention with reduced sequence length\n",
    "\n",
    "        :param x: tensor with shape (B, C, H, W) where\n",
    "            B is the Batch size\n",
    "            C is the number of Channels\n",
    "            H and W are the Height and Width\n",
    "        :return: tensor with shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        # Input BNC instead of BCHW\n",
    "        # BNC -> B,N,C*exp \n",
    "        x = self.mlp1(x)\n",
    "        B,N,C = x.shape\n",
    "        # Prepare for the CNN operation, channel should be 1st dim\n",
    "        # B,N, C*exp -> B, C*exp, H, W \n",
    "        x = x.transpose(1,2).view(B,C,H,W)\n",
    "\n",
    "        #Depth Conv - B, N, Cexp \n",
    "        x = self.gelu(self.depthwise(x).flatten(2).transpose(1,2))\n",
    "\n",
    "        #Back to the orignal shape\n",
    "        x = self.mlp2(x) # BNC\n",
    "        return x\n",
    "\n",
    "class MixTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, stride, padding, \n",
    "                 n_layers, reduction_ratio, num_heads, expansion_factor):\n",
    "        super().__init__()\n",
    "        self.patchMerge = OverlapPatchMerging(in_channels, out_channels, patch_size, stride, padding) # B N embed dim\n",
    "        #You might be wondering why I didn't used a cleaner implementation but the input to each forward function is different\n",
    "        self._attn = nn.ModuleList([EfficientSelfAttention(out_channels, reduction_ratio, num_heads) for _ in range(n_layers)])\n",
    "        self._ffn = nn.ModuleList([MixFFN(out_channels,expansion_factor) for _ in range(n_layers)])\n",
    "        self._lNorm = nn.ModuleList([nn.LayerNorm(out_channels) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run one block of the mix vision transformer\n",
    "\n",
    "        :param x: tensor with shape (B, C, H, W) where\n",
    "            B is the Batch size\n",
    "            C is the number of Channels\n",
    "            H and W are the Height and Width\n",
    "        :return: tensor with shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B,C,H,W = x.shape\n",
    "        x,H,W = self.patchMerge(x) # B N embed dim (C)\n",
    "        for i in range(len(self._attn)):\n",
    "            x = x + self._attn[i].forward(x, H, W) #BNC\n",
    "            x = x + self._ffn[i].forward(x, H, W) #BNC\n",
    "            x = self._lNorm[i].forward(x) #BNC\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous() #BCHW\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_inputs(X):\n",
    "\n",
    "    # fill quaternion rotation if not given\n",
    "    # make it [1, 0, 0, 0] repeated with numrows = X[0].shape[0]\n",
    "    if X[2] is None:\n",
    "        # X[2] = torch.Tensor([1, 0, 0, 0]).float()\n",
    "        X[2] = torch.zeros((X[0].shape[0], 4)).float().to(X[0].device)\n",
    "        X[2][:, 0] = 1\n",
    "\n",
    "    # if input depth images are not of right shape, resize\n",
    "    if X[0].shape[-2] != 60 or X[0].shape[-1] != 90:\n",
    "        X[0] = F.interpolate(X[0], size=(60, 90), mode='bilinear')\n",
    "\n",
    "    return X\n",
    "\n",
    "class LSTMNetVIT(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT+LSTM Network \n",
    "    Num Params: 3,563,663   \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            MixTransformerEncoderLayer(1, 32, patch_size=7, stride=4, padding=3, n_layers=2, reduction_ratio=8, num_heads=1, expansion_factor=8),\n",
    "            MixTransformerEncoderLayer(32, 64, patch_size=3, stride=2, padding=1, n_layers=2, reduction_ratio=4, num_heads=2, expansion_factor=8)\n",
    "        ])\n",
    "\n",
    "        self.decoder = spectral_norm(nn.Linear(4608, 512))\n",
    "        self.lstm = (nn.LSTM(input_size=517, hidden_size=128,\n",
    "                         num_layers=3, dropout=0.1))\n",
    "        self.nn_fc2 = spectral_norm(nn.Linear(128, 3))\n",
    "\n",
    "        self.up_sample = nn.Upsample(size=(16,24), mode='bilinear', align_corners=True)\n",
    "        self.pxShuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "        self.down_sample = nn.Conv2d(48,12,3, padding = 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = refine_inputs(X)\n",
    "\n",
    "        x = X[0]\n",
    "        embeds = [x]\n",
    "        for block in self.encoder_blocks:\n",
    "            embeds.append(block(embeds[-1]))        \n",
    "        out = embeds[1:]\n",
    "        out = torch.cat([self.pxShuffle(out[1]),self.up_sample(out[0])],dim=1) \n",
    "        out = self.down_sample(out)\n",
    "        out = self.decoder(out.flatten(1))\n",
    "        out = torch.cat([out, X[1]/10, X[2]], dim=1).float()\n",
    "        if len(X)>3:\n",
    "            out,h = self.lstm(out, X[3])\n",
    "        else:\n",
    "            out,h = self.lstm(out)\n",
    "        out = self.nn_fc2(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ITA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OverlapPatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(in_channels, out_channels, kernel_size=patch_size, stride = stride, padding = padding)\n",
    "        self.layerNorm = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        \"\"\"Merge patches to reduce dimensions of input.\n",
    "\n",
    "        :param patches: tensor with shape (B, C, H, W) where\n",
    "            B is the Batch size\n",
    "            C is the number of Channels\n",
    "            H and W are the Height and Width\n",
    "        :return: tensor with shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        x = self.cn1(patches)\n",
    "        _,_,H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1,2) #Flatten - (B,C,H*W); transpose B,HW, C\n",
    "        x = self.layerNorm(x)\n",
    "        return x,H,W #B, N, EmbedDim\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio, num_heads):\n",
    "        super().__init__()\n",
    "        assert channels % num_heads == 0, f\"channels {channels} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.heads= num_heads\n",
    "\n",
    "        #### Self Attention Block consists of 2 parts - Reduction and then normal Attention equation of queries and keys###\n",
    "        \n",
    "        # Reduction Parameters #\n",
    "        self.cn1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=reduction_ratio, stride= reduction_ratio)\n",
    "        self.ln1 = nn.LayerNorm(channels)\n",
    "        # Attention Parameters #\n",
    "        self.keyValueExtractor = nn.Linear(channels, channels * 2)\n",
    "        self.query = nn.Linear(channels, channels)\n",
    "        self.smax = nn.Softmax(dim=-1)\n",
    "        self.finalLayer = nn.Linear(channels, channels) \n",
    "\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "\n",
    "        \"\"\" Perform self attention with reduced sequence length\n",
    "\n",
    "        :param x: tensor of shape (B, N, C) where\n",
    "            B is the batch size,\n",
    "            N is the number of queries (equal to H * W)\n",
    "            C is the number of channels\n",
    "        :return: tensor of shape (B, N, C)\n",
    "        \"\"\"\n",
    "        B,N,C = x.shape\n",
    "        # B, N, C -> B, C, N\n",
    "        x1 = x.clone().permute(0,2,1)\n",
    "        # BCN -> BCHW\n",
    "        x1 = x1.reshape(B,C,H,W)\n",
    "        x1 = self.cn1(x1)\n",
    "        x1 = x1.reshape(B,C,-1).permute(0,2,1).contiguous()\n",
    "        x1 = self.ln1(x1)\n",
    "        # We have got the Reduced Embeddings! We need to extract key and value pairs now\n",
    "        keyVal = self.keyValueExtractor(x1)\n",
    "        keyVal = keyVal.reshape(B, -1 , 2, self.heads, int(C/self.heads)).permute(2,0,3,1,4).contiguous()\n",
    "        k,v = keyVal[0],keyVal[1] #b,heads, n, c/heads\n",
    "        q = self.query(x).reshape(B, N, self.heads, int(C/self.heads)).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        dimHead = (C/self.heads)**0.5\n",
    "        attention = self.smax(q@k.transpose(-2, -1)/dimHead)\n",
    "        attention = (attention@v).transpose(1,2).reshape(B,N,C)\n",
    "\n",
    "        x = self.finalLayer(attention) #B,N,C        \n",
    "        return x\n",
    "\n",
    "class MixFFN(nn.Module):\n",
    "    def __init__(self, channels, expansion_factor):\n",
    "        super().__init__()\n",
    "        expanded_channels = channels*expansion_factor\n",
    "        #MLP Layer        \n",
    "        self.mlp1 = nn.Linear(channels, expanded_channels)\n",
    "        #Depth Wise CNN Layer\n",
    "        self.depthwise = nn.Conv2d(expanded_channels, expanded_channels, kernel_size=3,  padding='same', groups=channels)\n",
    "        #GELU\n",
    "        self.gelu = nn.GELU()\n",
    "        #MLP to predict\n",
    "        self.mlp2 = nn.Linear(expanded_channels, channels)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Perform self attention with reduced sequence length\n",
    "\n",
    "        :param x: tensor with shape (B, C, H, W) where\n",
    "            B is the Batch size\n",
    "            C is the number of Channels\n",
    "            H and W are the Height and Width\n",
    "        :return: tensor with shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        # Input BNC instead of BCHW\n",
    "        # BNC -> B,N,C*exp \n",
    "        x = self.mlp1(x)\n",
    "        B,N,C = x.shape\n",
    "        # Prepare for the CNN operation, channel should be 1st dim\n",
    "        # B,N, C*exp -> B, C*exp, H, W \n",
    "        x = x.transpose(1,2).view(B,C,H,W)\n",
    "\n",
    "        #Depth Conv - B, N, Cexp \n",
    "        x = self.gelu(self.depthwise(x).flatten(2).transpose(1,2))\n",
    "\n",
    "        #Back to the orignal shape\n",
    "        x = self.mlp2(x) # BNC\n",
    "        return x\n",
    "\n",
    "class MixTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, stride, padding, \n",
    "                 n_layers, reduction_ratio, num_heads, expansion_factor):\n",
    "        super().__init__()\n",
    "        self.patchMerge = OverlapPatchMerging(in_channels, out_channels, patch_size, stride, padding) # B N embed dim\n",
    "        #You might be wondering why I didn't used a cleaner implementation but the input to each forward function is different\n",
    "        self._attn = nn.ModuleList([EfficientSelfAttention(out_channels, reduction_ratio, num_heads) for _ in range(n_layers)])\n",
    "        self._ffn = nn.ModuleList([MixFFN(out_channels,expansion_factor) for _ in range(n_layers)])\n",
    "        self._lNorm = nn.ModuleList([nn.LayerNorm(out_channels) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run one block of the mix vision transformer\n",
    "\n",
    "        :param x: tensor with shape (B, C, H, W) where\n",
    "            B is the Batch size\n",
    "            C is the number of Channels\n",
    "            H and W are the Height and Width\n",
    "        :return: tensor with shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B,C,H,W = x.shape\n",
    "        x,H,W = self.patchMerge(x) # B N embed dim (C)\n",
    "        for i in range(len(self._attn)):\n",
    "            x = x + self._attn[i].forward(x, H, W) #BNC\n",
    "            x = x + self._ffn[i].forward(x, H, W) #BNC\n",
    "            x = self._lNorm[i].forward(x) #BNC\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous() #BCHW\n",
    "        return x\n",
    "\n",
    "class MultiheadITAWithRequant(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, params=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        assert params is not None, \"Parameters for requantization must be provided\"\n",
    "\n",
    "        # Projections: shared across heads but produce concatenated head_dim output\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Requant parameters\n",
    "        self.params = params\n",
    "\n",
    "    def requant_shift(self, x, mult, shift):\n",
    "        x = x * mult\n",
    "        x = torch.div(x, 2 ** shift, rounding_mode='floor')\n",
    "        return torch.clamp(x + self.params[\"zp\"], -128, 127).to(torch.int8)\n",
    "\n",
    "    def forward(self, q_input, kv_input):\n",
    "        B_q, N_q, _ = q_input.shape\n",
    "        B_kv, N_kv, _ = kv_input.shape\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.q_proj(q_input).reshape(B_q, N_q, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(kv_input).reshape(B_kv, N_kv, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(kv_input).reshape(B_kv, N_kv, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = self.requant_shift(Q.to(torch.int32), self.params[\"mq\"], self.params[\"sq\"])\n",
    "        K = self.requant_shift(K.to(torch.int32), self.params[\"mk\"], self.params[\"sk\"])\n",
    "        V = self.requant_shift(V.to(torch.int32), self.params[\"mv\"], self.params[\"sv\"])\n",
    "\n",
    "        # Attention computation per head\n",
    "        Q = Q.permute(0, 2, 1, 3)  # (B, H, N, D)\n",
    "        K = K.permute(0, 2, 1, 3)\n",
    "        V = V.permute(0, 2, 1, 3)\n",
    "\n",
    "        if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "            Q = Q.to(torch.float32)\n",
    "            K = K.to(torch.float32)\n",
    "\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-2, -1))  # (B, H, N, N)\n",
    "        attn_logits = self.requant_shift(attn_logits, self.params[\"ma\"], self.params[\"sa\"])\n",
    "\n",
    "        attn_weights = ita_partial_max(attn_logits.float(), k=8)\n",
    "\n",
    "        context = torch.matmul(attn_weights, V.to(torch.float32))  # (B, H, N, D)\n",
    "        context = self.requant_shift(context, self.params[\"mav\"], self.params[\"sav\"])\n",
    "\n",
    "        # Concatenate all heads\n",
    "        context = context.permute(0, 2, 1, 3).reshape(B_q, N_q, self.embed_dim)\n",
    "\n",
    "        output = self.out_proj(context.to(torch.float32))  # Use float proj for now\n",
    "        output = self.requant_shift(output.to(torch.int32), self.params[\"mo\"], self.params[\"so\"])\n",
    "        final = self.requant_shift(output.to(torch.int32), self.params[\"mf\"], self.params[\"sf\"])\n",
    "\n",
    "        return final\n",
    "\n",
    "def ita_partial_max(logits: torch.Tensor, k: int = 8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Emulates ITAPartialMax by applying softmax to only the top-k elements along the last axis.\n",
    "    If k exceeds the dimension size, it is clipped.\n",
    "    \"\"\"\n",
    "    seq_len = logits.size(-1)\n",
    "    k = min(k, seq_len)  # Prevent topk from throwing\n",
    "    topk_vals, topk_indices = torch.topk(logits, k, dim=-1)\n",
    "    mask = torch.zeros_like(logits).scatter(-1, topk_indices, 1.0)\n",
    "    masked_logits = logits * mask\n",
    "    weights = F.softmax(masked_logits, dim=-1)\n",
    "    return weights\n",
    "\n",
    "class ITASelfAttentionWrapper(nn.Module):\n",
    "    def __init__(self, channels, embed_dim, num_heads, reduction_ratio, efficient_attn, itaparameters):\n",
    "        super().__init__()\n",
    "        # Reduction Parameters #\n",
    "        self.cn1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=reduction_ratio, stride= reduction_ratio)\n",
    "        self.ln1 = nn.LayerNorm(channels)\n",
    "        # Attention Parameters #\n",
    "        self.self_attn = MultiheadITAWithRequant(embed_dim=embed_dim, num_heads=num_heads, params=itaparameters)\n",
    "        self.efficient_attn = efficient_attn\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B,N,C = x.shape\n",
    "        # B, N, C -> B, C, N\n",
    "        # Optional spatial reduction for keys and values\n",
    "        if self.efficient_attn:\n",
    "            x1 = x.permute(0,2,1)\n",
    "            # BCN -> BCHW\n",
    "            x1 = x1.reshape(B,C,H,W)\n",
    "            x1 = self.cn1(x1)\n",
    "            x1 = x1.reshape(B,C,-1)\n",
    "            x1 = x1.permute(0,2,1).contiguous()\n",
    "            x1 = self.ln1(x1)\n",
    "\n",
    "            # Perform attention with (x as query, x1 as kv)\n",
    "            out = self.self_attn(x, x1)\n",
    "\n",
    "        else:\n",
    "            # Perform attention with (x as query and kv)\n",
    "            out = self.self_attn(x, x)\n",
    "        return out.float()\n",
    "    \n",
    "class MiXITAEncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, stride, padding, \n",
    "                 n_layers, reduction_ratio, num_heads, expansion_factor, embed_dim, efficient_attn=True, itaparameters=None):\n",
    "        super().__init__()\n",
    "        self.patchMerge = OverlapPatchMerging(in_channels, out_channels, patch_size, stride, padding) # B N embed dim\n",
    "        self._attn = nn.ModuleList([ITASelfAttentionWrapper(channels=out_channels,\n",
    "                                                            embed_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            reduction_ratio=reduction_ratio, \n",
    "                                                            efficient_attn=efficient_attn, \n",
    "                                                            itaparameters=itaparameters\n",
    "                                                            ) for _ in range(n_layers)])\n",
    "        self._ffn = nn.ModuleList([MixFFN(out_channels,expansion_factor) for _ in range(n_layers)])\n",
    "        self._lNorms = nn.ModuleList([nn.LayerNorm(out_channels) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        x,H,W = self.patchMerge(x) # B N embed dim (C)\n",
    "        for i in range(len(self._attn)):\n",
    "            x = x + self._attn[i].forward(x, H, W) #BNC\n",
    "            x = x + self._ffn[i].forward(x, H, W) #BNC  # Skip connections\n",
    "            x = self._lNorms[i].forward(x) #BNC\n",
    "        # Reshape tokens back to spatial format for next stage: (B, N, C) â†’ (B, C, H, W)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous() #BCHW\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITALSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    ITAConformer+LSTM Network\n",
    "    \"\"\"\n",
    "    def __init__(self, itaparameters=None, efficient_attn=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if itaparameters == None:\n",
    "            itaparameters = {\n",
    "            \"mq\": 1.0, \"sq\": 0, # multiplier_q and shift_q\n",
    "            \"mk\": 1.0, \"sk\": 0, # multiplier_k and shift_k\n",
    "            \"mv\": 1.0, \"sv\": 0, # multiplier_v and shift_v\n",
    "            \"ma\": 1.0, \"sa\": 0, # multiplier_attn and shift_attn\n",
    "            \"mav\": 1.0, \"sav\": 0, # multiplier_av and shift_av\n",
    "            \"mo\": 1.0, \"so\": 0, # multiplier_o and shift_o\n",
    "            \"mf\": 1.0, \"sf\": 0, # multiplier_final and shift_final\n",
    "            \"zp\": 0, # zero_point\n",
    "            }\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            MiXITAEncoderLayer(1, 32, patch_size=7, stride=4, padding=3,\n",
    "                               n_layers=2, reduction_ratio=8, num_heads=1,\n",
    "                               expansion_factor=8, embed_dim=32,\n",
    "                               efficient_attn=efficient_attn, itaparameters=itaparameters),\n",
    "            MiXITAEncoderLayer(32, 64, patch_size=3, stride=2, padding=1,\n",
    "                               n_layers=2, reduction_ratio=4, num_heads=2,\n",
    "                               expansion_factor=8, embed_dim=64,\n",
    "                               efficient_attn=efficient_attn, itaparameters=itaparameters)\n",
    "        ])\n",
    "\n",
    "        self.decoder = spectral_norm(nn.Linear(4608, 512))\n",
    "        self.lstm = nn.LSTM(input_size=517, hidden_size=128,\n",
    "                            num_layers=3, dropout=0.1)\n",
    "        self.nn_fc2 = spectral_norm(nn.Linear(128, 3))\n",
    "\n",
    "        self.up_sample = nn.Upsample(size=(16, 24), mode='bilinear', align_corners=True)\n",
    "        self.pxShuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "        self.down_sample = nn.Conv2d(48, 12, 3, padding=1)\n",
    "\n",
    "    def _encode(self, x):\n",
    "        embeds = [x]\n",
    "        for block in self.encoder_blocks:\n",
    "            embeds.append(block(embeds[-1]))\n",
    "        return embeds[1:]\n",
    "\n",
    "    def _decode(self, encoded_features):\n",
    "        out = torch.cat([self.pxShuffle(encoded_features[1]), self.up_sample(encoded_features[0])], dim=1)\n",
    "        out = self.down_sample(out)\n",
    "        return self.decoder(out.flatten(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = refine_inputs(X)\n",
    "        x = X[0]\n",
    "\n",
    "        encoded_features = self._encode(x)\n",
    "        out = self._decode(encoded_features)\n",
    "        # Each out[i]: (T, C, H, W)\n",
    "        # Flatten per timestep: (T, 12, 16, 24) -> (T, 4608)\n",
    "        # Concat additional inputs: (T, 512) + (1, T, 1) + (1, T, 4) => (T, 517)\n",
    "        out = torch.cat([out, X[1].squeeze(0)/10, X[2].squeeze(0)], dim=1).float()\n",
    "        if len(X) > 3:\n",
    "            out, h = self.lstm(out, X[3])\n",
    "        else:\n",
    "            out, h = self.lstm(out)\n",
    "        out = self.nn_fc2(out)\n",
    "        return out, h\n",
    "\n",
    "\n",
    "\n",
    "class ITAConformer(nn.Module):   \n",
    "    def __init__(self, itaparameters=None, efficient_attn=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if itaparameters == None:\n",
    "            itaparameters = {\n",
    "            \"mq\": 1.0, \"sq\": 0, # multiplier_q and shift_q\n",
    "            \"mk\": 1.0, \"sk\": 0, # multiplier_k and shift_k\n",
    "            \"mv\": 1.0, \"sv\": 0, # multiplier_v and shift_v\n",
    "            \"ma\": 1.0, \"sa\": 0, # multiplier_attn and shift_attn\n",
    "            \"mav\": 1.0, \"sav\": 0, # multiplier_av and shift_av\n",
    "            \"mo\": 1.0, \"so\": 0, # multiplier_o and shift_o\n",
    "            \"mf\": 1.0, \"sf\": 0, # multiplier_final and shift_final\n",
    "            \"zp\": 0, # zero_point\n",
    "            }\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            MiXITAEncoderLayer(1, 32, patch_size=7, stride=4, padding=3,\n",
    "                               n_layers=2, reduction_ratio=8, num_heads=1,\n",
    "                               expansion_factor=8, embed_dim=32,\n",
    "                               efficient_attn=efficient_attn, itaparameters=itaparameters),\n",
    "            MiXITAEncoderLayer(32, 64, patch_size=3, stride=2, padding=1,\n",
    "                               n_layers=2, reduction_ratio=4, num_heads=2,\n",
    "                               expansion_factor=8, embed_dim=64,\n",
    "                               efficient_attn=efficient_attn, itaparameters=itaparameters)\n",
    "        ])       \n",
    "        self.decoder = nn.Linear(4608, 512)\n",
    "        self.nn_fc1 = spectral_norm(nn.Linear(517, 256))\n",
    "        self.nn_fc2 = spectral_norm(nn.Linear(256, 3))\n",
    "        self.up_sample = nn.Upsample(size=(16,24), mode='bilinear', align_corners=True)\n",
    "        self.pxShuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "        self.down_sample = nn.Conv2d(48,12,3, padding = 1)\n",
    "\n",
    "    def _encode(self, x):\n",
    "        embeds = [x]\n",
    "        for block in self.encoder_blocks:\n",
    "            embeds.append(block(embeds[-1]))\n",
    "        return embeds[1:]\n",
    "\n",
    "    def _decode(self, encoded_features):\n",
    "        out = torch.cat([self.pxShuffle(encoded_features[1]), self.up_sample(encoded_features[0])], dim=1)\n",
    "        out = self.down_sample(out)\n",
    "        return self.decoder(out.flatten(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = refine_inputs(X)\n",
    "\n",
    "        x = X[0]\n",
    "        encoded_features = self._encode(x)\n",
    "        out = self._decode(encoded_features)\n",
    "        out = torch.cat([out, X[1]/10, X[2]], dim=1).float()\n",
    "        out = F.leaky_relu(self.nn_fc1(out))\n",
    "        out = self.nn_fc2(out)\n",
    "\n",
    "        return out, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AOT Compilation for pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed iree-turbine, Version: 3.2.0\n",
      "\n",
      "Installed IREE, compiler version information:\n",
      "IREE (https://iree.dev):\n",
      "  IREE compiler version 3.2.0rc20250206 @ f3bef2de123f08b4fc3b0ce691494891bd6760d0\n",
      "  LLVM version 20.0.0git\n",
      "  Optimized build\n",
      "\n",
      "Installed PyTorch, version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "!echo \"Installed iree-turbine, $(python -m pip show iree_turbine | grep Version)\"\n",
    "\n",
    "!echo -e \"\\nInstalled IREE, compiler version information:\"\n",
    "!iree-compile --version\n",
    "\n",
    "import torch\n",
    "print(\"\\nInstalled PyTorch, version:\", torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IREE_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
