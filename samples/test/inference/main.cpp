// src/test_inference.cpp
#include <iostream>
#include <vector>
#include <cassert>
#include <numeric> // For std::iota

// --- IREE C API Headers ---
#include "iree/runtime/api.h"
#include "iree/hal/api.h"
#include "iree/hal/drivers/local_sync/sync_device.h"
#include "iree/hal/local/loaders/static_library_loader.h"

// --- Headers Generated by IREE (Names defined in CMakeLists.txt) ---
#include "ITAViTLSTM_c.h"
#include "ITAViTLSTM_static_lib.h"

#define IREE_CHECK_OK(status) assert(iree_status_is_ok(status))

// --- Forward Declarations ---
iree_status_t create_device_with_static_loader(iree_allocator_t host_allocator, iree_hal_device_t** out_device);
iree_status_t create_tensor_view(iree_hal_device_t* device, const void* data, const iree_hal_dim_t* shape, iree_host_size_t shape_rank, iree_hal_element_type_t element_type, iree_hal_buffer_view_t** out_buffer_view);
void print_output_tensor(iree_hal_buffer_view_t* view);

// --- Main Application ---
int main() {
    std::cout << "--- Running Self-Contained IREE Inference Test ---" << std::endl;

    // --- 1. IREE Setup (Identical to server) ---
    iree_runtime_instance_options_t instance_options;
    iree_runtime_instance_options_initialize(&instance_options);
    iree_runtime_instance_t* instance = NULL;
    IREE_CHECK_OK(iree_runtime_instance_create(&instance_options, iree_allocator_system(), &instance));

    iree_hal_device_t* device = NULL;
    IREE_CHECK_OK(create_device_with_static_loader(iree_runtime_instance_host_allocator(instance), &device));

    iree_runtime_session_options_t session_options;
    iree_runtime_session_options_initialize(&session_options);
    iree_runtime_session_t* session = NULL;
    IREE_CHECK_OK(iree_runtime_session_create_with_device(instance, &session_options, device, iree_runtime_instance_host_allocator(instance), &session));

    // --- 2. Load the Bytecode Module (Identical to server) ---
    const struct iree_file_toc_t* module_file = ITAViTLSTM_create();
    iree_vm_module_t* bytecode_module = NULL;
    IREE_CHECK_OK(iree_vm_bytecode_module_create(
        iree_runtime_instance_vm_instance(instance),
        iree_make_const_byte_span(module_file->data, module_file->size),
        iree_allocator_null(), iree_allocator_system(), &bytecode_module));
    IREE_CHECK_OK(iree_runtime_session_append_module(session, bytecode_module));
    iree_vm_module_release(bytecode_module);

    // --- 3. Lookup the Inference Function (Identical to server) ---
    const char* func_name = "module.main_graph";
    iree_runtime_call_t call;
    IREE_CHECK_OK(iree_runtime_call_initialize_by_name(session, iree_make_cstring_view(func_name), &call));

    // --- 4. Initialize Hidden State Tensors (Identical to server) ---
    const iree_hal_dim_t hidden_shape[] = {3, 1, 128};
    const size_t hidden_byte_size = 3 * 1 * 128 * sizeof(float);
    std::vector<char> zero_buffer(hidden_byte_size, 0);
    iree_hal_buffer_view_t* hidden_state_h = NULL;
    IREE_CHECK_OK(create_tensor_view(device, zero_buffer.data(), hidden_shape, 3, IREE_HAL_ELEMENT_TYPE_FLOAT_32, &hidden_state_h));
    iree_hal_buffer_view_t* hidden_state_c = NULL;
    IREE_CHECK_OK(create_tensor_view(device, zero_buffer.data(), hidden_shape, 3, IREE_HAL_ELEMENT_TYPE_FLOAT_32, &hidden_state_c));

    // --- 5. Create Dummy Input Data (Replaces Networking) ---
    std::cout << "Creating dummy input data for one inference call..." << std::endl;
    // Input 0: Image (1x1x60x90, float32)
    std::vector<float> image_f32(1 * 1 * 60 * 90);
    std::iota(image_f32.begin(), image_f32.end(), 0.0f); // Fill with 0.0, 1.0, 2.0, ...
    for(auto& val : image_f32) val /= (60*90); // Normalize to ~[0,1]
    const iree_hal_dim_t img_shape[] = {1, 1, 60, 90};
    iree_hal_buffer_view_t* img_view = NULL;
    IREE_CHECK_OK(create_tensor_view(device, image_f32.data(), img_shape, 4, IREE_HAL_ELEMENT_TYPE_FLOAT_32, &img_view));

    // Input 1: Desired Velocity (1x1, float32)
    float desired_velocity = 5.0f;
    const iree_hal_dim_t vel_shape[] = {1, 1};
    iree_hal_buffer_view_t* vel_view = NULL;
    IREE_CHECK_OK(create_tensor_view(device, &desired_velocity, vel_shape, 2, IREE_HAL_ELEMENT_TYPE_FLOAT_32, &vel_view));

    // Input 2: Quaternion (1x4, float32)
    float quaternion[] = {1.0f, 0.0f, 0.0f, 0.0f};
    const iree_hal_dim_t quat_shape[] = {1, 4};
    iree_hal_buffer_view_t* quat_view = NULL;
    IREE_CHECK_OK(create_tensor_view(device, quaternion, quat_shape, 2, IREE_HAL_ELEMENT_TYPE_FLOAT_32, &quat_view));

    // --- 6. Prepare and Invoke ---
    iree_runtime_call_inputs_clear(&call);
    IREE_CHECK_OK(iree_runtime_call_inputs_push_back_buffer_view(&call, img_view));
    IREE_CHECK_OK(iree_runtime_call_inputs_push_back_buffer_view(&call, vel_view));
    IREE_CHECK_OK(iree_runtime_call_inputs_push_back_buffer_view(&call, quat_view));
    IREE_CHECK_OK(iree_runtime_call_inputs_push_back_buffer_view(&call, hidden_state_h));
    IREE_CHECK_OK(iree_runtime_call_inputs_push_back_buffer_view(&call, hidden_state_c));

    std::cout << "Invoking the model..." << std::endl;
    IREE_CHECK_OK(iree_runtime_call_invoke(&call, /*flags=*/0));
    std::cout << "Invocation complete." << std::endl;

    // --- 7. Verify Outputs (Replaces Sending Reply) ---
    iree_hal_buffer_view_t* raw_output_view = NULL;
    IREE_CHECK_OK(iree_runtime_call_outputs_pop_front_buffer_view(&call, &raw_output_view));
    iree_hal_buffer_view_t* new_hidden_state_h = NULL;
    IREE_CHECK_OK(iree_runtime_call_outputs_pop_front_buffer_view(&call, &new_hidden_state_h));
    iree_hal_buffer_view_t* new_hidden_state_c = NULL;
    IREE_CHECK_OK(iree_runtime_call_outputs_pop_front_buffer_view(&call, &new_hidden_state_c));

    std::cout << "\n--- Inference Results ---" << std::endl;
    print_output_tensor(raw_output_view);
    std::cout << "-------------------------" << std::endl;

    // --- 8. Final Cleanup ---
    iree_hal_buffer_view_release(img_view);
    iree_hal_buffer_view_release(vel_view);
    iree_hal_buffer_view_release(quat_view);
    iree_hal_buffer_view_release(hidden_state_h);
    iree_hal_buffer_view_release(hidden_state_c);
    iree_hal_buffer_view_release(raw_output_view);
    iree_hal_buffer_view_release(new_hidden_state_h);
    iree_hal_buffer_view_release(new_hidden_state_c);

    iree_runtime_call_deinitialize(&call);
    iree_runtime_session_release(session);
    iree_hal_device_release(device);
    iree_runtime_instance_release(instance);

    std::cout << "\nTest passed successfully." << std::endl;
    return 0;
}


// --- Helper Implementations ---
// (These are identical to the server versions)

iree_status_t create_device_with_static_loader(iree_allocator_t host_allocator, iree_hal_device_t** out_device) {
    iree_hal_sync_device_params_t params;
    iree_hal_sync_device_params_initialize(&params);

    const iree_hal_executable_library_query_fn_t libraries[] = { ITAViTLSTM_static_lib_library_query };
    iree_hal_executable_loader_t* static_loader = NULL;
    IREE_CHECK_OK(iree_hal_static_library_loader_create(IREE_ARRAYSIZE(libraries), libraries, iree_hal_executable_import_provider_null(), host_allocator, &static_loader));

    iree_string_view_t identifier = iree_make_cstring_view("local-sync");
    iree_hal_allocator_t* device_allocator = NULL;
    IREE_CHECK_OK(iree_hal_allocator_create_heap(identifier, host_allocator, host_allocator, &device_allocator));
    
    IREE_CHECK_OK(iree_hal_sync_device_create(identifier, &params, 1, &static_loader, device_allocator, host_allocator, out_device));
    
    iree_hal_allocator_release(device_allocator);
    iree_hal_executable_loader_release(static_loader);
    return iree_ok_status();
}

iree_status_t create_tensor_view(iree_hal_device_t* device, const void* data, const iree_hal_dim_t* shape, iree_host_size_t shape_rank, iree_hal_element_type_t element_type, iree_hal_buffer_view_t** out_buffer_view) {
    iree_hal_buffer_params_t buffer_params = {0};
    buffer_params.type = IREE_HAL_MEMORY_TYPE_DEVICE_LOCAL;
    buffer_params.usage = IREE_HAL_BUFFER_USAGE_DEFAULT;
    iree_host_size_t byte_length = 0;
    IREE_CHECK_OK(iree_hal_buffer_view_compute_size_and_stride(shape_rank, shape, element_type, IREE_HAL_ENCODING_TYPE_DENSE_ROW_MAJOR, &byte_length, NULL));
    return iree_hal_buffer_view_allocate_buffer_copy(device, iree_hal_device_allocator(device), shape_rank, shape, element_type, IREE_HAL_ENCODING_TYPE_DENSE_ROW_MAJOR, buffer_params, iree_make_const_byte_span(data, byte_length), out_buffer_view);
}

void print_output_tensor(iree_hal_buffer_view_t* view) {
    if (!view) {
        std::cout << "  <null>" << std::endl;
        return;
    }

    // Map the buffer so we can read it on the host
    iree_hal_buffer_mapping_t mapped_memory;
    IREE_CHECK_OK(iree_hal_buffer_map_range(iree_hal_buffer_view_buffer(view), IREE_HAL_MAPPING_MODE_SCOPED,
                                             IREE_HAL_MEMORY_ACCESS_READ, 0, IREE_WHOLE_BUFFER, &mapped_memory));

    const float* data_ptr = reinterpret_cast<const float*>(mapped_memory.contents.data);
    iree_host_size_t element_count = iree_hal_buffer_view_element_count(view);

    std::cout << "  Output Tensor (Shape: ";
    for (int i = 0; i < iree_hal_buffer_view_shape_rank(view); ++i) {
        std::cout << iree_hal_buffer_view_shape_dim(view, i) << (i < iree_hal_buffer_view_shape_rank(view) - 1 ? "x" : "");
    }
    std::cout << ", Type: f32):" << std::endl;
    
    std::cout << "  [";
    for (iree_host_size_t i = 0; i < element_count; ++i) {
        std::cout << data_ptr[i] << (i < element_count - 1 ? ", " : "");
    }
    std::cout << "]" << std::endl;

    iree_hal_buffer_unmap_range(&mapped_memory);
}